{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einops-inspired indexing\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/indexing.png\" width=\"350\">\n",
    "\n",
    "There are a few libraries which have made functions like this, but here is my own interpretation of what indexing would look like with einops-style notation.\n",
    "\n",
    "The idea is for the pattern string you pass to the `eindex` function to be as close as possible to how you'd think about defining each element of the output tensor. For example, suppose you have a logprobs tensor of shape `(batch_size, seq_len, d_vocab)`, and a tensor of correct next tokens with shape `(batch_size, seq_len)`, and you want to get the logprobs on the correct tokens. You would think about this as follows:\n",
    "\n",
    "```\n",
    "output[batch, seq] = logprobs[batch, seq, labels[batch, seq]]\n",
    "```\n",
    "\n",
    "This is implemented essentially by essentially writing the thing inside the right hand square brackets expression as the pattern string:\n",
    "\n",
    "```python\n",
    "output = eindex(logprobs, labels, \"batch seq [batch seq]\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install eindex-callum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing logprobs\n",
    "\n",
    "Here is the example given above (along with a few other ways you could get the same result, and showing that they're equivalent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from eindex import eindex\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 5\n",
    "D_VOCAB = 100\n",
    "\n",
    "logprobs = torch.randn(BATCH_SIZE, SEQ_LEN, D_VOCAB).log_softmax(-1)\n",
    "labels = torch.randint(0, D_VOCAB, (BATCH_SIZE, SEQ_LEN))\n",
    "\n",
    "# (1) Using eindex\n",
    "output_1 = eindex(logprobs, labels, \"batch seq [batch seq]\")\n",
    "\n",
    "# (2) Normal PyTorch, using `gather`\n",
    "output_2 = logprobs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# (3) Normal PyTorch, not using `gather` (this is like what `eindex` does under the hood)\n",
    "output_3 = logprobs[torch.arange(BATCH_SIZE).unsqueeze(-1), torch.arange(SEQ_LEN), labels]\n",
    "\n",
    "# Check they're all the same\n",
    "assert torch.allclose(output_1, output_2)\n",
    "assert torch.allclose(output_1, output_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple index dimensions\n",
    "\n",
    "Suppose that your output vocab shape was 2D rather than 1D (weird I know but bear with me), and your labels tensor had shape `(batch_size, seq_len, 2)` (i.e. each slice corresponded to a different dimension of the output vocab). You want to index the following:\n",
    "\n",
    "```python\n",
    "output[batch, seq, d1, d2] = logprobs[batch, seq, labels[batch, seq, 0], labels[batch, seq, 1]]\n",
    "```\n",
    "\n",
    "Again, this is implemented just like it's written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_VOCAB_1 = 100\n",
    "D_VOCAB_2 = 50\n",
    "\n",
    "logprobs = torch.randn(BATCH_SIZE, SEQ_LEN, D_VOCAB_1, D_VOCAB_2).log_softmax(-1)\n",
    "labels = torch.stack([\n",
    "    torch.randint(0, D_VOCAB_1, (BATCH_SIZE, SEQ_LEN)), \n",
    "    torch.randint(0, D_VOCAB_2, (BATCH_SIZE, SEQ_LEN))\n",
    "], dim=-1)\n",
    "\n",
    "# (1) Using eindex\n",
    "output_1 = eindex(logprobs, labels, \"batch seq [batch seq 0] [batch seq 1]\")\n",
    "\n",
    "# (2) Normal PyTorch, using `gather` (apparently GPT4 couldn't come up with anything less janky)\n",
    "combined_index = labels[..., 0] * D_VOCAB_2 + labels[..., 1]\n",
    "logprobs_flattened = logprobs.view(BATCH_SIZE, SEQ_LEN, D_VOCAB_1 * D_VOCAB_2)\n",
    "output_2 = logprobs_flattened.gather(2, combined_index.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# (3) Normal PyTorch, not using `gather`\n",
    "output_3 = logprobs[torch.arange(BATCH_SIZE)[:, None], torch.arange(SEQ_LEN)[None, :], labels[:, :, 0], labels[:, :, 1]]\n",
    "\n",
    "# Check they're all the same\n",
    "assert torch.allclose(output_1, output_2)\n",
    "assert torch.allclose(output_1, output_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had 2 different labels (rather than 2 different dimensions of the same label), this is also supported. We want to index the tensor as:\n",
    "\n",
    "```\n",
    "output[batch, seq, d1, d2] = logprobs[batch, seq, labels_1[batch, seq], labels_2[batch, seq]]\n",
    "```\n",
    "\n",
    "and this is implemented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = torch.randn(BATCH_SIZE, SEQ_LEN, D_VOCAB_1, D_VOCAB_2).log_softmax(-1)\n",
    "labels_1 = torch.randint(0, D_VOCAB_1, (BATCH_SIZE, SEQ_LEN))\n",
    "labels_2 = torch.randint(0, D_VOCAB_2, (BATCH_SIZE, SEQ_LEN))\n",
    "\n",
    "# (1) Using eindex\n",
    "output_1 = eindex(logprobs, labels_1, labels_2, \"batch seq [batch seq] [batch seq]\")\n",
    "\n",
    "# (2) Normal PyTorch, using `gather`\n",
    "combined_index = labels_1 * D_VOCAB_2 + labels_2\n",
    "logprobs_flattened = logprobs.view(BATCH_SIZE, SEQ_LEN, D_VOCAB_1 * D_VOCAB_2)\n",
    "output_2 = logprobs_flattened.gather(2, combined_index.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# (3) Normal PyTorch, not using `gather`\n",
    "output_3 = logprobs[torch.arange(BATCH_SIZE)[:, None], torch.arange(SEQ_LEN)[None, :], labels_1, labels_2]\n",
    "\n",
    "# Check they're all the same\n",
    "assert torch.allclose(output_1, output_2)\n",
    "assert torch.allclose(output_1, output_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - when using multiple tensors, the square brackets are assumed to refer to the index tensors in the order they appear.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offsetting dimensions\n",
    "\n",
    "Let's go back to our logprobs and labels example earlier. Assume our labels are tokens in an autoregressive transformer. Usually, we'd have `logprobs` and `tokens` in a form such that they'd need to be offset by one, i.e. we want the tensor:\n",
    "\n",
    "```\n",
    "output[batch, seq] = logprobs[batch, seq, tokens[batch, seq+1]]\n",
    "```\n",
    "\n",
    "which has shape `(batch_size, seq_len-1)`. \n",
    "\n",
    "Using the tools so far, we could implement this by just slicing `logprobs` and `tokens` before doing the eindexing operation shown in the very first example:\n",
    "\n",
    "```python\n",
    "output = eindex(logprobs[:, :-1], tokens[:, 1:], \"batch seq [batch seq]\")\n",
    "```\n",
    "\n",
    "However, there's also a way to perform this slicing within the indexing function itself:\n",
    "\n",
    "```python\n",
    "output = eindex(logprobs, tokens, \"batch seq [batch seq+1]\")\n",
    "```\n",
    "\n",
    "This functionality is definitely more on the \"optional\" side, because I can imagine most users might prefer to do the slicing themselves. However, it seemed an intuitive extension to offer so I thought I'd include it!\n",
    "\n",
    "**Note** - you shouldn't have a space around the `+` sign!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = torch.randn(BATCH_SIZE, SEQ_LEN, D_VOCAB).log_softmax(-1)\n",
    "tokens = torch.randint(0, D_VOCAB, (BATCH_SIZE, SEQ_LEN))\n",
    "\n",
    "# (1) Using eindex directly\n",
    "output_1 = eindex(logprobs, tokens, \"batch seq [batch seq+1]\")\n",
    "\n",
    "# (1) Using eindex plus slicing first\n",
    "output_2 = eindex(logprobs[:, :-1], tokens[:, 1:], \"batch seq [batch seq]\")\n",
    "\n",
    "# Check they're the same\n",
    "assert output_1.shape == (BATCH_SIZE, SEQ_LEN - 1)\n",
    "assert torch.allclose(output_1, output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error-checking\n",
    "\n",
    "I've tried to make the errors in this library as informative as possible. For example, if you use the same named dimension twice in the pattern string but it corresponds to different length dimensions, the exact mistake will be printed out. Here are 2 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incompatible sizes for 'batch' dimension.\n",
      "Based on your inputs, the inferred dimension sizes are 'batch=32 seq=5 [batch=33 seq=5]'.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "BATCH_SIZE_INCORRECT = 33\n",
    "\n",
    "logprobs = torch.randn(BATCH_SIZE, SEQ_LEN, D_VOCAB).log_softmax(-1)\n",
    "labels = torch.randint(0, D_VOCAB, (BATCH_SIZE_INCORRECT, SEQ_LEN))\n",
    "\n",
    "try:\n",
    "    output = eindex(logprobs, labels, \"batch seq [batch seq]\")\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid indices.\n",
      "Number of terms in your string pattern = 2\n",
      "Number of terms in your array to index into = 3\n",
      "These should match.\n"
     ]
    }
   ],
   "source": [
    "logprobs = torch.randn(BATCH_SIZE, SEQ_LEN, D_VOCAB).log_softmax(-1)\n",
    "labels = torch.randint(0, D_VOCAB, (BATCH_SIZE, SEQ_LEN))\n",
    "\n",
    "try:\n",
    "    output = eindex(logprobs, labels, \"batch [batch seq]\")\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you come across any other errors which seem common enough that they should have more readable error messages, please let me know! This library is very small and easy to maintain (it's mostly just one function), so I'm pretty likely to be able to implement most small changes if I think they'd improve the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra examples\n",
    "\n",
    "These don't introduce any new functionality, but I'm including them in this notebook because they've all failed in the past even when all the examples above succeeded. If they pass, I can be more confident that changes I make aren't breaking changes. Hopefully these cover a wide enough range of use cases to serve this purpose (I might add to them going forwards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[batch, d_vocab] = logprobs[batch, token_indices[batch], d_vocab]\n",
    "\n",
    "logprobs = torch.randn(BATCH_SIZE, SEQ_LEN, D_VOCAB).log_softmax(-1)\n",
    "token_indices = torch.randint(0, SEQ_LEN, (BATCH_SIZE,))\n",
    "\n",
    "output = eindex(logprobs, token_indices, \"batch [batch] d_vocab\")\n",
    "\n",
    "assert output.shape == (BATCH_SIZE, D_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[batch, seqQ, k] = tokens[batch, token_indices[batch, seqQ, k]]\n",
    "\n",
    "K = 5\n",
    "tokens = torch.randint(0, SEQ_LEN, (BATCH_SIZE, SEQ_LEN))\n",
    "token_indices = torch.randint(0, SEQ_LEN, (BATCH_SIZE, SEQ_LEN, K))\n",
    "\n",
    "output = eindex(tokens, token_indices, \"batch [batch seqQ k]\")\n",
    "\n",
    "assert output.shape == (BATCH_SIZE, SEQ_LEN, K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_intro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
